{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72db4518-0902-43d0-a38a-dafefd32927f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb # Already imported, keeping for context if you still use LGBM later\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor # Make sure you have xgboost installed (pip install xgboost)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import joblib # To save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e6d8604-61f6-4230-bebd-4714a7600919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Load All Preprocessed Data ---\n",
      "Loaded X_multi_modal shape: (4359, 2074)\n",
      "Loaded y_multi_modal shape: (4359,)\n",
      "Loaded X_tabular_only shape: (5952, 26)\n",
      "Loaded y_tabular_only shape: (5952,)\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Step 1: Load All Preprocessed Data ---\")\n",
    "\n",
    "# Load multi-modal data (from df_hybrid)\n",
    "try:\n",
    "    X_multi_modal = pd.read_csv('processed_diamond_features_X_multi_modal.csv')\n",
    "    y_multi_modal = pd.read_csv('diamond_target_y_multi_model.csv').squeeze() # .squeeze() to ensure it's a Series\n",
    "    print(f\"Loaded X_multi_modal shape: {X_multi_modal.shape}\")\n",
    "    print(f\"Loaded y_multi_modal shape: {y_multi_modal.shape}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"ERROR: Multi-modal files not found. Ensure 'processed_diamond_features_X_multi_modal.csv' and 'diamond_target_y.csv' exist.\")\n",
    "    print(e)\n",
    "    exit()\n",
    "\n",
    "# Load tabular-only data (from df_tabular_only_strategy)\n",
    "try:\n",
    "    X_tabular_only = pd.read_csv('processed_diamond_features_X.csv')\n",
    "    y_tabular_only = pd.read_csv('diamond_target_y.csv').squeeze() # .squeeze() to ensure it's a Series\n",
    "    print(f\"Loaded X_tabular_only shape: {X_tabular_only.shape}\")\n",
    "    print(f\"Loaded y_tabular_only shape: {y_tabular_only.shape}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"ERROR: Tabular-only files not found. Ensure 'processed_diamond_features_X_tabular_only.csv' and 'diamond_target_y_tabular_only.csv' exist.\")\n",
    "    print(e)\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0595e4e9-075c-44b5-ac1c-b3dd7157323e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 2: Concatenate Features (X) and Targets (y) ---\n",
      "Final combined X_final shape: (10311, 2074)\n",
      "Final combined y_final shape: (10311,)\n",
      "Sample of final X_final (head):\n",
      "   Clarity  Colour  Colour_IsFancy_0  Colour_IsFancy_1  Cut  Fluorescence_F  \\\n",
      "0      5.0     3.0               1.0               0.0  3.0             0.0   \n",
      "1      3.0     7.0               1.0               0.0  3.0             1.0   \n",
      "2      4.0     7.0               1.0               0.0  2.0             0.0   \n",
      "3      6.0     7.0               1.0               0.0  3.0             1.0   \n",
      "4      4.0     2.0               1.0               0.0  2.0             0.0   \n",
      "\n",
      "   Fluorescence_M  Fluorescence_N  Fluorescence_SL  Fluorescence_ST  ...  \\\n",
      "0             1.0             0.0              0.0              0.0  ...   \n",
      "1             0.0             0.0              0.0              0.0  ...   \n",
      "2             0.0             1.0              0.0              0.0  ...   \n",
      "3             0.0             0.0              0.0              0.0  ...   \n",
      "4             0.0             1.0              0.0              0.0  ...   \n",
      "\n",
      "   img_feature_990  img_feature_991  img_feature_992  img_feature_993  \\\n",
      "0         0.013109         0.923034         0.488516         0.099008   \n",
      "1         0.021429         0.580545         2.381295         0.087748   \n",
      "2         0.000000         1.242322         2.094914         1.109092   \n",
      "3         0.332028         0.913823         1.817700         0.060134   \n",
      "4         0.133198         1.212584         0.591125         0.533967   \n",
      "\n",
      "   img_feature_994  img_feature_995  img_feature_996  img_feature_997  \\\n",
      "0         0.012866         3.383372         0.000011         0.311224   \n",
      "1         0.074311         4.403018         0.000000         0.372938   \n",
      "2         0.006773         4.573275         0.000000         0.166476   \n",
      "3         0.000000         1.980968         0.000000         0.208715   \n",
      "4         0.058795         5.793454         0.000000         0.544995   \n",
      "\n",
      "   img_feature_998  img_feature_999  \n",
      "0         0.000000         0.076283  \n",
      "1         0.074014         0.073138  \n",
      "2         0.066381         0.254017  \n",
      "3         0.000000         0.021767  \n",
      "4         0.000000         0.533661  \n",
      "\n",
      "[5 rows x 2074 columns]\n",
      "\n",
      "Sample of final X_final (tail, to see tabular-only data):\n",
      "       Clarity  Colour  Colour_IsFancy_0  Colour_IsFancy_1  Cut  \\\n",
      "10306      4.0    16.0               1.0               0.0  2.0   \n",
      "10307      3.0    12.0               1.0               0.0  3.0   \n",
      "10308      5.0    12.0               1.0               0.0  3.0   \n",
      "10309      2.0    16.0               1.0               0.0  2.0   \n",
      "10310      2.0    16.0               1.0               0.0  3.0   \n",
      "\n",
      "       Fluorescence_F  Fluorescence_M  Fluorescence_N  Fluorescence_SL  \\\n",
      "10306             0.0             0.0             1.0              0.0   \n",
      "10307             0.0             0.0             1.0              0.0   \n",
      "10308             0.0             0.0             1.0              0.0   \n",
      "10309             0.0             0.0             1.0              0.0   \n",
      "10310             0.0             0.0             1.0              0.0   \n",
      "\n",
      "       Fluorescence_ST  ...  img_feature_990  img_feature_991  \\\n",
      "10306              0.0  ...              0.0              0.0   \n",
      "10307              0.0  ...              0.0              0.0   \n",
      "10308              0.0  ...              0.0              0.0   \n",
      "10309              0.0  ...              0.0              0.0   \n",
      "10310              0.0  ...              0.0              0.0   \n",
      "\n",
      "       img_feature_992  img_feature_993  img_feature_994  img_feature_995  \\\n",
      "10306              0.0              0.0              0.0              0.0   \n",
      "10307              0.0              0.0              0.0              0.0   \n",
      "10308              0.0              0.0              0.0              0.0   \n",
      "10309              0.0              0.0              0.0              0.0   \n",
      "10310              0.0              0.0              0.0              0.0   \n",
      "\n",
      "       img_feature_996  img_feature_997  img_feature_998  img_feature_999  \n",
      "10306              0.0              0.0              0.0              0.0  \n",
      "10307              0.0              0.0              0.0              0.0  \n",
      "10308              0.0              0.0              0.0              0.0  \n",
      "10309              0.0              0.0              0.0              0.0  \n",
      "10310              0.0              0.0              0.0              0.0  \n",
      "\n",
      "[5 rows x 2074 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Step 2: Concatenate Features (X) and Targets (y) ---\")\n",
    "\n",
    "# Ensure columns are aligned before concatenation.\n",
    "# This is crucial if one-hot encoding produced different columns due to varying categories in subsets.\n",
    "# We'll use reindex to make sure all X dataframes have the same columns, filling missing with 0.\n",
    "all_columns = pd.Index(X_multi_modal.columns).union(X_tabular_only.columns)\n",
    "\n",
    "X_multi_modal_aligned = X_multi_modal.reindex(columns=all_columns, fill_value=0)\n",
    "X_tabular_only_aligned = X_tabular_only.reindex(columns=all_columns, fill_value=0)\n",
    "\n",
    "\n",
    "# Concatenate the feature DataFrames vertically\n",
    "X_final = pd.concat([X_multi_modal_aligned, X_tabular_only_aligned], ignore_index=True)\n",
    "\n",
    "# Concatenate the target Series vertically (order must match X)\n",
    "y_final = pd.concat([y_multi_modal, y_tabular_only], ignore_index=True)\n",
    "\n",
    "print(f\"Final combined X_final shape: {X_final.shape}\")\n",
    "print(f\"Final combined y_final shape: {y_final.shape}\")\n",
    "print(\"Sample of final X_final (head):\")\n",
    "print(X_final.head())\n",
    "print(\"\\nSample of final X_final (tail, to see tabular-only data):\")\n",
    "print(X_final.tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18cc521a-773f-49ca-bbb2-14f905bc6db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 1.1. Descriptive Statistics of Diamond Prices (Target Variable) ---\n",
      "count    10311.000000\n",
      "mean      1694.120854\n",
      "std       1592.725457\n",
      "min        512.460000\n",
      "25%        940.740000\n",
      "50%       1319.660000\n",
      "75%       2042.450000\n",
      "max      16751.620000\n",
      "Name: Price, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Added section to describe target variable\n",
    "print(\"\\n--- 1.1. Descriptive Statistics of Diamond Prices (Target Variable) ---\")\n",
    "print(y_final.describe())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e728b83-fdf7-46ac-ac1e-fdaf83cea89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_transformed = np.log1p(y_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae8155e0-8457-44a3-b62d-f40253631dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 3: Split Data into Training and Testing Sets ---\n",
      "X_train shape: (8248, 2074)\n",
      "X_test shape: (2063, 2074)\n",
      "y_train shape: (8248,)\n",
      "y_test shape: (2063,)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Step 3: Split Data into Training and Testing Sets ---\")\n",
    "\n",
    "# Split the combined data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_final, Y_transformed, test_size=0.2, random_state=37 # 20% for testing, use random_state for reproducibility\n",
    ")\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e03525c-cb32-4afa-b0cc-df45093acd54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 4. Defining Models and Hyperparameter Grids ---\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- 4. Defining Models and Hyperparameter Grids ---\")\n",
    "param_grids = {\n",
    "    \"Decision Tree\": {\n",
    "        \"max_depth\": [5, 20, None],\n",
    "        \"min_samples_split\": [2, 10],\n",
    "        \"min_samples_leaf\": [1, 5]\n",
    "    },\n",
    "    \"Random Forest\": {\n",
    "        \"n_estimators\": [50, 200], # Added 200 as a common value\n",
    "        \"max_depth\": [10, None], # Added 20\n",
    "        \"min_samples_split\": [2, 10], # Added 10\n",
    "        \"min_samples_leaf\": [2, 4] # Added 4\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        \"n_estimators\": [100, 200, 300], # Adjusted values\n",
    "        \"learning_rate\": [0.01, 0.05, 0.2], # Adjusted values\n",
    "        \"max_depth\": [3, 5, 10], # Adjusted values\n",
    "        \"subsample\": [0.6, 0.8, 1.0], # Adjusted values\n",
    "        \"colsample_bytree\": [0.6, 0.8, 1.0] # Added colsample_bytree for more comprehensive search\n",
    "    },\n",
    "    \"LightGBM\": { # Adding LightGBM as a candidate too!\n",
    "        \"n_estimators\": [100, 200, 300],\n",
    "        \"learning_rate\": [0.01, 0.1, 0.2],\n",
    "        \"num_leaves\": [31, 63, 127],\n",
    "        \"max_depth\": [-1, 7, 15], # -1 means no limit\n",
    "        \"reg_alpha\": [0, 0.1, 0.5],\n",
    "        \"reg_lambda\": [0, 0.1, 0.5]\n",
    "    }\n",
    "}\n",
    "\n",
    "models = {\n",
    "    \"Decision Tree\": DecisionTreeRegressor(random_state=42),\n",
    "    \"Random Forest\": RandomForestRegressor(random_state=42),\n",
    "    \"XGBoost\": XGBRegressor(random_state=42, objective='reg:squarederror', eval_metric='rmse'), # Default objective for regression, eval_metric for consistency\n",
    "    \"LightGBM\": lgb.LGBMRegressor(random_state=42, objective='regression_l1') # Using MAE objective like before\n",
    "}\n",
    "\n",
    "best_models = {}\n",
    "evaluation_results = {} # To store test set results for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cb0b8a6-74c5-4916-8073-492ac29cec50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 4. Performing Hyperparameter Tuning with RandomizedSearchCV ---\n",
      "\n",
      "Tuning Decision Tree...\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Best parameters for Decision Tree: {'min_samples_split': 10, 'min_samples_leaf': 5, 'max_depth': 20}\n",
      "Best R2 score on validation sets for Decision Tree: 0.9425\n",
      "Test Set Evaluation for Decision Tree: RMSE = 0.11, R2 = 0.9547\n",
      "\n",
      "Tuning Random Forest...\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Best parameters for Random Forest: {'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_depth': None}\n",
      "Best R2 score on validation sets for Random Forest: 0.9660\n",
      "Test Set Evaluation for Random Forest: RMSE = 0.08, R2 = 0.9720\n",
      "\n",
      "Tuning XGBoost...\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:528: FitFailedWarning: \n",
      "9 fits failed out of a total of 30.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\xgboost\\sklearn.py\", line 1247, in fit\n",
      "    self._Booster = train(\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\xgboost\\training.py\", line 183, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\xgboost\\core.py\", line 2246, in update\n",
      "    _check_call(\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\xgboost\\core.py\", line 310, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: [20:59:32] C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\io.h:389: bad_malloc: Failed to allocate 829407744 bytes.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\xgboost\\sklearn.py\", line 1247, in fit\n",
      "    self._Booster = train(\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\xgboost\\training.py\", line 183, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\xgboost\\core.py\", line 2246, in update\n",
      "    _check_call(\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\xgboost\\core.py\", line 310, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: [20:59:35] C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\io.h:389: bad_malloc: Failed to allocate 1264923376 bytes.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\xgboost\\sklearn.py\", line 1247, in fit\n",
      "    self._Booster = train(\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\xgboost\\training.py\", line 183, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\xgboost\\core.py\", line 2246, in update\n",
      "    _check_call(\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\xgboost\\core.py\", line 310, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: [20:59:32] C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\io.h:389: bad_malloc: Failed to allocate 795944960 bytes.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\xgboost\\sklearn.py\", line 1247, in fit\n",
      "    self._Booster = train(\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\xgboost\\training.py\", line 183, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\xgboost\\core.py\", line 2246, in update\n",
      "    _check_call(\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\xgboost\\core.py\", line 310, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: [20:59:33] C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\io.h:389: bad_malloc: Failed to allocate 1231544832 bytes.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\xgboost\\sklearn.py\", line 1247, in fit\n",
      "    self._Booster = train(\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\xgboost\\training.py\", line 183, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\xgboost\\core.py\", line 2246, in update\n",
      "    _check_call(\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\xgboost\\core.py\", line 310, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: [20:59:38] C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\io.h:389: bad_malloc: Failed to allocate 2052359120 bytes.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\xgboost\\sklearn.py\", line 1247, in fit\n",
      "    self._Booster = train(\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\xgboost\\training.py\", line 183, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\xgboost\\core.py\", line 2246, in update\n",
      "    _check_call(\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\xgboost\\core.py\", line 310, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: [20:59:34] C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\io.h:389: bad_malloc: Failed to allocate 1047296000 bytes.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\xgboost\\sklearn.py\", line 1247, in fit\n",
      "    self._Booster = train(\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\xgboost\\training.py\", line 183, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\xgboost\\core.py\", line 2246, in update\n",
      "    _check_call(\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\xgboost\\core.py\", line 310, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: [20:59:31] C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\io.h:389: bad_malloc: Failed to allocate 846163456 bytes.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\xgboost\\sklearn.py\", line 1247, in fit\n",
      "    self._Booster = train(\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\xgboost\\training.py\", line 183, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\xgboost\\core.py\", line 2246, in update\n",
      "    _check_call(\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\xgboost\\core.py\", line 310, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: [20:59:41] C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\io.h:389: bad_malloc: Failed to allocate 1566494512 bytes.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\xgboost\\sklearn.py\", line 1247, in fit\n",
      "    self._Booster = train(\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\xgboost\\training.py\", line 183, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\xgboost\\core.py\", line 2246, in update\n",
      "    _check_call(\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\xgboost\\core.py\", line 310, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: [20:59:45] C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\io.h:389: bad_malloc: Failed to allocate 2304051200 bytes.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the test scores are non-finite: [0.96645409        nan 0.95859368 0.87496444        nan        nan\n",
      " 0.96534624 0.9228576  0.94386096 0.9221752 ]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for XGBoost: {'subsample': 1.0, 'n_estimators': 200, 'max_depth': 3, 'learning_rate': 0.2, 'colsample_bytree': 1.0}\n",
      "Best R2 score on validation sets for XGBoost: 0.9665\n",
      "Test Set Evaluation for XGBoost: RMSE = 0.08, R2 = 0.9722\n",
      "\n",
      "Tuning LightGBM...\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread ExecutorManagerThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\psutil\\_pswindows.py\", line 688, in wrapper\n",
      "    return fun(self, *args, **kwargs)\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\psutil\\_pswindows.py\", line 872, in kill\n",
      "    return cext.proc_kill(self.pid)\n",
      "PermissionError: [WinError 5] Access is denied: '(originated from OpenProcess)'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 608, in run\n",
      "    self.flag_executor_shutting_down()\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 826, in flag_executor_shutting_down\n",
      "    self.kill_workers(reason=\"executor shutting down\")\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 836, in kill_workers\n",
      "    kill_process_tree(p)\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\joblib\\externals\\loky\\backend\\utils.py\", line 19, in kill_process_tree\n",
      "    _kill_process_tree_with_psutil(process)\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\joblib\\externals\\loky\\backend\\utils.py\", line 43, in _kill_process_tree_with_psutil\n",
      "    descendant.kill()\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\psutil\\__init__.py\", line 278, in wrapper\n",
      "    return fun(self, *args, **kwargs)\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\psutil\\__init__.py\", line 1248, in kill\n",
      "    self._proc.kill()\n",
      "  File \"C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\psutil\\_pswindows.py\", line 690, in wrapper\n",
      "    raise convert_oserror(err, pid=self.pid, name=self._name)\n",
      "psutil.AccessDenied: (pid=24300)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error tuning LightGBM: Unable to allocate 43.0 MiB for an array with shape (2048, 2750) and data type float64\n",
      "Skipping this model and moving to the next.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- 4. Performing Hyperparameter Tuning with RandomizedSearchCV ---\")\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nTuning {model_name}...\")\n",
    "    grid_search = RandomizedSearchCV(model, param_grids[model_name], cv=3, scoring='r2', n_jobs=-1, verbose=1, n_iter=10, random_state=37)\n",
    "\n",
    "    try:\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        best_models[model_name] = grid_search.best_estimator_\n",
    "        print(f\"Best parameters for {model_name}: {grid_search.best_params_}\")\n",
    "        print(f\"Best R2 score on validation sets for {model_name}: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "        # Evaluate the best estimator on the test set\n",
    "        y_pred = best_models[model_name].predict(X_test)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        evaluation_results[model_name] = {'RMSE': rmse, 'R2': r2}\n",
    "        print(f\"Test Set Evaluation for {model_name}: RMSE = {rmse:.2f}, R2 = {r2:.4f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error tuning {model_name}: {e}\")\n",
    "        print(\"Skipping this model and moving to the next.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1166b520-affa-4874-8d8d-eca1eaa420a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 5. Adding Linear Regression (No Hyperparameter Tuning) ---\n",
      "Test Set Evaluation for Linear Regression: RMSE = 0.17, R2 = 0.8786\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- 5. Adding Linear Regression (No Hyperparameter Tuning) ---\")\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "best_models[\"Linear Regression\"] = lr\n",
    "\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "rmse_lr = np.sqrt(mean_squared_error(y_test, y_pred_lr))\n",
    "r2_lr = r2_score(y_test, y_pred_lr)\n",
    "evaluation_results[\"Linear Regression\"] = {'RMSE': rmse_lr, 'R2': r2_lr}\n",
    "print(f\"Test Set Evaluation for Linear Regression: RMSE = {rmse_lr:.2f}, R2 = {r2_lr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "280cc641-eeb2-4df8-8c89-776cd03d45e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, StackingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "774b1938-590b-4172-883d-307601bdfb70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 6. Implementing and Evaluating Stacking Regressor ---\n",
      "Starting Stacking Regressor training...\n",
      "Stacking Regressor training complete.\n",
      "Test Set Evaluation for Stacking Regressor: RMSE = 0.08, R2 = 0.9760\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- 6. Implementing and Evaluating Stacking Regressor ---\")\n",
    "\n",
    "estimators = [\n",
    "    ('dt', best_models[\"Decision Tree\"]),\n",
    "    ('rf', best_models[\"Random Forest\"]),\n",
    "    ('xgb', best_models[\"XGBoost\"])\n",
    "]\n",
    "\n",
    "final_estimator = Ridge(alpha=1.0)\n",
    "\n",
    "stacking_regressor = StackingRegressor(\n",
    "    estimators=estimators,\n",
    "    final_estimator=final_estimator,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Starting Stacking Regressor training...\")\n",
    "stacking_regressor.fit(X_train, y_train)\n",
    "print(\"Stacking Regressor training complete.\")\n",
    "\n",
    "y_pred_stack = stacking_regressor.predict(X_test)\n",
    "rmse_stack = np.sqrt(mean_squared_error(y_test, y_pred_stack))\n",
    "r2_stack = r2_score(y_test, y_pred_stack)\n",
    "\n",
    "best_models[\"Stacking Regressor\"] = stacking_regressor\n",
    "evaluation_results[\"Stacking Regressor\"] = {'RMSE': rmse_stack, 'R2': r2_stack}\n",
    "print(f\"Test Set Evaluation for Stacking Regressor: RMSE = {rmse_stack:.2f}, R2 = {r2_stack:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a3db3ee-89eb-468c-9cce-cb2d5e5f2547",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam # Using Adam optimizer\n",
    "from keras.callbacks import EarlyStopping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f4fd12a-1140-48fa-8da8-c35f01adac41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 7. Implementing and Evaluating Deep Neural Network (DNN) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">531,200</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │         \u001b[38;5;34m531,200\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m32,896\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │           \u001b[38;5;34m8,256\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m65\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">572,417</span> (2.18 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m572,417\u001b[0m (2.18 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">572,417</span> (2.18 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m572,417\u001b[0m (2.18 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting DNN model training...\n",
      "Epoch 1/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 13.3104 - mae: 2.7715 - root_mean_squared_error: 3.4585 - val_loss: 2.9019 - val_mae: 1.4600 - val_root_mean_squared_error: 1.7035\n",
      "Epoch 2/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2.9101 - mae: 1.3611 - root_mean_squared_error: 1.7055 - val_loss: 2.6541 - val_mae: 1.2139 - val_root_mean_squared_error: 1.6292\n",
      "Epoch 3/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2.5755 - mae: 1.2908 - root_mean_squared_error: 1.6046 - val_loss: 3.6327 - val_mae: 1.4974 - val_root_mean_squared_error: 1.9060\n",
      "Epoch 4/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2.1556 - mae: 1.1746 - root_mean_squared_error: 1.4681 - val_loss: 3.0966 - val_mae: 1.5792 - val_root_mean_squared_error: 1.7597\n",
      "Epoch 5/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2.0467 - mae: 1.1458 - root_mean_squared_error: 1.4299 - val_loss: 2.3230 - val_mae: 1.3310 - val_root_mean_squared_error: 1.5241\n",
      "Epoch 6/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.7940 - mae: 1.0653 - root_mean_squared_error: 1.3388 - val_loss: 1.9835 - val_mae: 1.0025 - val_root_mean_squared_error: 1.4084\n",
      "Epoch 7/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.6403 - mae: 1.0313 - root_mean_squared_error: 1.2807 - val_loss: 1.8904 - val_mae: 1.0057 - val_root_mean_squared_error: 1.3749\n",
      "Epoch 8/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.4584 - mae: 0.9566 - root_mean_squared_error: 1.2074 - val_loss: 1.6916 - val_mae: 1.1528 - val_root_mean_squared_error: 1.3006\n",
      "Epoch 9/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.3165 - mae: 0.9102 - root_mean_squared_error: 1.1473 - val_loss: 1.2332 - val_mae: 0.8675 - val_root_mean_squared_error: 1.1105\n",
      "Epoch 10/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.2264 - mae: 0.8822 - root_mean_squared_error: 1.1073 - val_loss: 0.4157 - val_mae: 0.4761 - val_root_mean_squared_error: 0.6447\n",
      "Epoch 11/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.0993 - mae: 0.8340 - root_mean_squared_error: 1.0484 - val_loss: 0.3940 - val_mae: 0.4879 - val_root_mean_squared_error: 0.6277\n",
      "Epoch 12/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.0121 - mae: 0.7972 - root_mean_squared_error: 1.0060 - val_loss: 0.2170 - val_mae: 0.3405 - val_root_mean_squared_error: 0.4659\n",
      "Epoch 13/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.9382 - mae: 0.7721 - root_mean_squared_error: 0.9686 - val_loss: 0.1191 - val_mae: 0.2937 - val_root_mean_squared_error: 0.3451\n",
      "Epoch 14/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.8874 - mae: 0.7509 - root_mean_squared_error: 0.9419 - val_loss: 0.2018 - val_mae: 0.3311 - val_root_mean_squared_error: 0.4492\n",
      "Epoch 15/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.8507 - mae: 0.7307 - root_mean_squared_error: 0.9223 - val_loss: 0.1265 - val_mae: 0.2693 - val_root_mean_squared_error: 0.3556\n",
      "Epoch 16/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.8355 - mae: 0.7236 - root_mean_squared_error: 0.9139 - val_loss: 0.1092 - val_mae: 0.2543 - val_root_mean_squared_error: 0.3304\n",
      "Epoch 17/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7948 - mae: 0.7050 - root_mean_squared_error: 0.8914 - val_loss: 0.2807 - val_mae: 0.4183 - val_root_mean_squared_error: 0.5298\n",
      "Epoch 18/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7718 - mae: 0.7002 - root_mean_squared_error: 0.8784 - val_loss: 0.1218 - val_mae: 0.2646 - val_root_mean_squared_error: 0.3490\n",
      "Epoch 19/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.7259 - mae: 0.6783 - root_mean_squared_error: 0.8519 - val_loss: 0.1062 - val_mae: 0.2421 - val_root_mean_squared_error: 0.3259\n",
      "Epoch 20/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6915 - mae: 0.6627 - root_mean_squared_error: 0.8315 - val_loss: 0.1302 - val_mae: 0.2731 - val_root_mean_squared_error: 0.3608\n",
      "Epoch 21/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6617 - mae: 0.6454 - root_mean_squared_error: 0.8133 - val_loss: 0.0958 - val_mae: 0.2282 - val_root_mean_squared_error: 0.3095\n",
      "Epoch 22/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6224 - mae: 0.6244 - root_mean_squared_error: 0.7887 - val_loss: 0.1148 - val_mae: 0.2736 - val_root_mean_squared_error: 0.3388\n",
      "Epoch 23/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.5797 - mae: 0.5983 - root_mean_squared_error: 0.7611 - val_loss: 0.0945 - val_mae: 0.2227 - val_root_mean_squared_error: 0.3074\n",
      "Epoch 24/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.5803 - mae: 0.6034 - root_mean_squared_error: 0.7616 - val_loss: 0.0990 - val_mae: 0.2510 - val_root_mean_squared_error: 0.3146\n",
      "Epoch 25/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.5584 - mae: 0.5853 - root_mean_squared_error: 0.7472 - val_loss: 0.1437 - val_mae: 0.3295 - val_root_mean_squared_error: 0.3790\n",
      "Epoch 26/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.5295 - mae: 0.5718 - root_mean_squared_error: 0.7275 - val_loss: 0.1201 - val_mae: 0.2789 - val_root_mean_squared_error: 0.3465\n",
      "Epoch 27/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5017 - mae: 0.5552 - root_mean_squared_error: 0.7081 - val_loss: 0.1027 - val_mae: 0.2435 - val_root_mean_squared_error: 0.3204\n",
      "Epoch 28/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.4645 - mae: 0.5389 - root_mean_squared_error: 0.6815 - val_loss: 0.0816 - val_mae: 0.2304 - val_root_mean_squared_error: 0.2857\n",
      "Epoch 29/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.4716 - mae: 0.5409 - root_mean_squared_error: 0.6867 - val_loss: 0.0741 - val_mae: 0.1966 - val_root_mean_squared_error: 0.2723\n",
      "Epoch 30/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.4431 - mae: 0.5207 - root_mean_squared_error: 0.6652 - val_loss: 0.1165 - val_mae: 0.2663 - val_root_mean_squared_error: 0.3414\n",
      "Epoch 31/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.4106 - mae: 0.5016 - root_mean_squared_error: 0.6406 - val_loss: 0.1033 - val_mae: 0.2668 - val_root_mean_squared_error: 0.3214\n",
      "Epoch 32/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3834 - mae: 0.4827 - root_mean_squared_error: 0.6191 - val_loss: 0.0720 - val_mae: 0.2232 - val_root_mean_squared_error: 0.2684\n",
      "Epoch 33/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3702 - mae: 0.4770 - root_mean_squared_error: 0.6083 - val_loss: 0.1163 - val_mae: 0.2785 - val_root_mean_squared_error: 0.3410\n",
      "Epoch 34/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3446 - mae: 0.4573 - root_mean_squared_error: 0.5870 - val_loss: 0.1104 - val_mae: 0.2601 - val_root_mean_squared_error: 0.3322\n",
      "Epoch 35/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.3305 - mae: 0.4431 - root_mean_squared_error: 0.5748 - val_loss: 0.0729 - val_mae: 0.2118 - val_root_mean_squared_error: 0.2699\n",
      "Epoch 36/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3076 - mae: 0.4305 - root_mean_squared_error: 0.5545 - val_loss: 0.0791 - val_mae: 0.2125 - val_root_mean_squared_error: 0.2813\n",
      "Epoch 37/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2905 - mae: 0.4170 - root_mean_squared_error: 0.5389 - val_loss: 0.0786 - val_mae: 0.2063 - val_root_mean_squared_error: 0.2804\n",
      "Epoch 38/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2901 - mae: 0.4151 - root_mean_squared_error: 0.5385 - val_loss: 0.0736 - val_mae: 0.2150 - val_root_mean_squared_error: 0.2712\n",
      "Epoch 39/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2803 - mae: 0.4089 - root_mean_squared_error: 0.5294 - val_loss: 0.0779 - val_mae: 0.2104 - val_root_mean_squared_error: 0.2791\n",
      "Epoch 40/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2606 - mae: 0.3890 - root_mean_squared_error: 0.5104 - val_loss: 0.0714 - val_mae: 0.2066 - val_root_mean_squared_error: 0.2673\n",
      "Epoch 41/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2405 - mae: 0.3768 - root_mean_squared_error: 0.4903 - val_loss: 0.0985 - val_mae: 0.2353 - val_root_mean_squared_error: 0.3139\n",
      "Epoch 42/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2271 - mae: 0.3635 - root_mean_squared_error: 0.4765 - val_loss: 0.0700 - val_mae: 0.1933 - val_root_mean_squared_error: 0.2645\n",
      "Epoch 43/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2065 - mae: 0.3500 - root_mean_squared_error: 0.4543 - val_loss: 0.0603 - val_mae: 0.1746 - val_root_mean_squared_error: 0.2456\n",
      "Epoch 44/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1966 - mae: 0.3359 - root_mean_squared_error: 0.4433 - val_loss: 0.0627 - val_mae: 0.1842 - val_root_mean_squared_error: 0.2504\n",
      "Epoch 45/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1939 - mae: 0.3314 - root_mean_squared_error: 0.4402 - val_loss: 0.0740 - val_mae: 0.2031 - val_root_mean_squared_error: 0.2720\n",
      "Epoch 46/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1686 - mae: 0.3107 - root_mean_squared_error: 0.4105 - val_loss: 0.0681 - val_mae: 0.1893 - val_root_mean_squared_error: 0.2610\n",
      "Epoch 47/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1653 - mae: 0.3070 - root_mean_squared_error: 0.4065 - val_loss: 0.0622 - val_mae: 0.1824 - val_root_mean_squared_error: 0.2494\n",
      "Epoch 48/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1648 - mae: 0.3087 - root_mean_squared_error: 0.4058 - val_loss: 0.0722 - val_mae: 0.2029 - val_root_mean_squared_error: 0.2687\n",
      "Epoch 49/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1593 - mae: 0.2960 - root_mean_squared_error: 0.3991 - val_loss: 0.0522 - val_mae: 0.1723 - val_root_mean_squared_error: 0.2284\n",
      "Epoch 50/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1465 - mae: 0.2831 - root_mean_squared_error: 0.3827 - val_loss: 0.0642 - val_mae: 0.1809 - val_root_mean_squared_error: 0.2533\n",
      "DNN model training complete.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- 7. Implementing and Evaluating Deep Neural Network (DNN) ---\")\n",
    "\n",
    "# Define the DNN model architecture\n",
    "def build_dnn_model(input_shape):\n",
    "    model = Sequential([\n",
    "        # Input layer and first hidden layer\n",
    "        Dense(256, activation='relu', input_shape=(input_shape,)),\n",
    "        Dropout(0.4), # Dropout for regularization\n",
    "        # Second hidden layer\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        # Third hidden layer\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.4),\n",
    "        # Output layer for regression (single neuron, no activation)\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    # Using Adam optimizer with a custom learning rate\n",
    "    # Loss: Mean Squared Error (MSE) is common for regression\n",
    "    # Metrics: RMSE and MAE (Mean Absolute Error) are good to monitor\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=[tf.keras.metrics.RootMeanSquaredError(), 'mae'])\n",
    "    return model\n",
    "\n",
    "# Get the input shape from our training data\n",
    "input_dim = X_train.shape[1]\n",
    "dnn_model = build_dnn_model(input_dim)\n",
    "\n",
    "# Print model summary\n",
    "dnn_model.summary()\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',         # Metric to monitor (validation loss)\n",
    "    patience=3,                 # Number of epochs with no improvement after which training will be stopped\n",
    "    mode='min',                 # 'min' because we want to minimize the loss\n",
    "    restore_best_weights=True,  # Restores model weights from the epoch with the best value of the monitored metric.\n",
    "    verbose=1                   # Show messages when stopping\n",
    ")\n",
    "# --- End Early Stopping Callback ---\n",
    "\n",
    "# Train the DNN model\n",
    "print(\"\\nStarting DNN model training...\")\n",
    "# Using 50 epochs, a batch size of 32, and validating on the test set\n",
    "history = dnn_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test),\n",
    "    verbose=1, # Show training progress\n",
    "    #callbacks = [early_stopping]\n",
    ")\n",
    "print(\"DNN model training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "258bc0ae-7cef-4f67-8a2d-fb54e51643ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1481 - mae: 0.2817 - root_mean_squared_error: 0.3847 - val_loss: 0.0570 - val_mae: 0.1786 - val_root_mean_squared_error: 0.2387\n",
      "Epoch 2/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1386 - mae: 0.2771 - root_mean_squared_error: 0.3721 - val_loss: 0.0603 - val_mae: 0.1843 - val_root_mean_squared_error: 0.2455\n",
      "Epoch 3/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1239 - mae: 0.2626 - root_mean_squared_error: 0.3519 - val_loss: 0.0643 - val_mae: 0.1871 - val_root_mean_squared_error: 0.2535\n",
      "Epoch 4/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1189 - mae: 0.2508 - root_mean_squared_error: 0.3448 - val_loss: 0.0709 - val_mae: 0.1931 - val_root_mean_squared_error: 0.2662\n",
      "Epoch 5/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1235 - mae: 0.2514 - root_mean_squared_error: 0.3511 - val_loss: 0.0628 - val_mae: 0.1797 - val_root_mean_squared_error: 0.2507\n",
      "Epoch 6/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1115 - mae: 0.2396 - root_mean_squared_error: 0.3338 - val_loss: 0.0589 - val_mae: 0.1841 - val_root_mean_squared_error: 0.2428\n",
      "Epoch 7/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1077 - mae: 0.2334 - root_mean_squared_error: 0.3278 - val_loss: 0.0643 - val_mae: 0.1891 - val_root_mean_squared_error: 0.2535\n",
      "Epoch 8/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1069 - mae: 0.2263 - root_mean_squared_error: 0.3267 - val_loss: 0.0563 - val_mae: 0.1729 - val_root_mean_squared_error: 0.2373\n",
      "Epoch 9/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1067 - mae: 0.2278 - root_mean_squared_error: 0.3265 - val_loss: 0.0477 - val_mae: 0.1570 - val_root_mean_squared_error: 0.2184\n",
      "Epoch 10/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0925 - mae: 0.2130 - root_mean_squared_error: 0.3040 - val_loss: 0.0528 - val_mae: 0.1680 - val_root_mean_squared_error: 0.2299\n",
      "Epoch 11/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0930 - mae: 0.2095 - root_mean_squared_error: 0.3048 - val_loss: 0.0489 - val_mae: 0.1593 - val_root_mean_squared_error: 0.2211\n",
      "Epoch 12/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1017 - mae: 0.2109 - root_mean_squared_error: 0.3188 - val_loss: 0.0658 - val_mae: 0.1865 - val_root_mean_squared_error: 0.2565\n",
      "Epoch 13/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0920 - mae: 0.2069 - root_mean_squared_error: 0.3030 - val_loss: 0.0827 - val_mae: 0.2108 - val_root_mean_squared_error: 0.2876\n",
      "Epoch 14/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0861 - mae: 0.2018 - root_mean_squared_error: 0.2933 - val_loss: 0.0609 - val_mae: 0.1701 - val_root_mean_squared_error: 0.2469\n",
      "Epoch 15/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0888 - mae: 0.1962 - root_mean_squared_error: 0.2977 - val_loss: 0.0624 - val_mae: 0.1824 - val_root_mean_squared_error: 0.2498\n",
      "Epoch 16/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0888 - mae: 0.1964 - root_mean_squared_error: 0.2978 - val_loss: 0.0636 - val_mae: 0.1806 - val_root_mean_squared_error: 0.2522\n",
      "Epoch 17/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0883 - mae: 0.1947 - root_mean_squared_error: 0.2970 - val_loss: 0.0513 - val_mae: 0.1618 - val_root_mean_squared_error: 0.2266\n",
      "Epoch 18/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0821 - mae: 0.1888 - root_mean_squared_error: 0.2864 - val_loss: 0.0611 - val_mae: 0.1781 - val_root_mean_squared_error: 0.2471\n",
      "Epoch 19/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0866 - mae: 0.1913 - root_mean_squared_error: 0.2941 - val_loss: 0.0679 - val_mae: 0.1852 - val_root_mean_squared_error: 0.2606\n",
      "Epoch 20/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0847 - mae: 0.1868 - root_mean_squared_error: 0.2905 - val_loss: 0.0599 - val_mae: 0.1775 - val_root_mean_squared_error: 0.2447\n",
      "Epoch 21/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0773 - mae: 0.1839 - root_mean_squared_error: 0.2779 - val_loss: 0.0572 - val_mae: 0.1714 - val_root_mean_squared_error: 0.2392\n",
      "Epoch 22/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0886 - mae: 0.1924 - root_mean_squared_error: 0.2976 - val_loss: 0.0484 - val_mae: 0.1555 - val_root_mean_squared_error: 0.2200\n",
      "Epoch 23/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0738 - mae: 0.1806 - root_mean_squared_error: 0.2714 - val_loss: 0.0679 - val_mae: 0.1850 - val_root_mean_squared_error: 0.2606\n",
      "Epoch 24/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0769 - mae: 0.1822 - root_mean_squared_error: 0.2771 - val_loss: 0.0595 - val_mae: 0.1735 - val_root_mean_squared_error: 0.2440\n",
      "Epoch 25/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0769 - mae: 0.1832 - root_mean_squared_error: 0.2771 - val_loss: 0.0513 - val_mae: 0.1592 - val_root_mean_squared_error: 0.2264\n",
      "Epoch 26/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0815 - mae: 0.1837 - root_mean_squared_error: 0.2855 - val_loss: 0.0518 - val_mae: 0.1619 - val_root_mean_squared_error: 0.2276\n",
      "Epoch 27/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0887 - mae: 0.1906 - root_mean_squared_error: 0.2977 - val_loss: 0.0668 - val_mae: 0.1888 - val_root_mean_squared_error: 0.2585\n",
      "Epoch 28/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0783 - mae: 0.1822 - root_mean_squared_error: 0.2796 - val_loss: 0.0559 - val_mae: 0.1652 - val_root_mean_squared_error: 0.2365\n",
      "Epoch 29/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0859 - mae: 0.1873 - root_mean_squared_error: 0.2927 - val_loss: 0.0524 - val_mae: 0.1601 - val_root_mean_squared_error: 0.2290\n",
      "Epoch 30/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0734 - mae: 0.1798 - root_mean_squared_error: 0.2707 - val_loss: 0.0655 - val_mae: 0.1800 - val_root_mean_squared_error: 0.2560\n",
      "Epoch 31/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0800 - mae: 0.1813 - root_mean_squared_error: 0.2827 - val_loss: 0.0466 - val_mae: 0.1467 - val_root_mean_squared_error: 0.2159\n",
      "Epoch 32/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0790 - mae: 0.1824 - root_mean_squared_error: 0.2806 - val_loss: 0.0520 - val_mae: 0.1579 - val_root_mean_squared_error: 0.2280\n",
      "Epoch 33/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0780 - mae: 0.1803 - root_mean_squared_error: 0.2792 - val_loss: 0.0416 - val_mae: 0.1444 - val_root_mean_squared_error: 0.2040\n",
      "Epoch 34/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0813 - mae: 0.1798 - root_mean_squared_error: 0.2851 - val_loss: 0.0526 - val_mae: 0.1605 - val_root_mean_squared_error: 0.2294\n",
      "Epoch 35/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0794 - mae: 0.1797 - root_mean_squared_error: 0.2818 - val_loss: 0.0609 - val_mae: 0.1713 - val_root_mean_squared_error: 0.2468\n",
      "Epoch 36/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0839 - mae: 0.1847 - root_mean_squared_error: 0.2893 - val_loss: 0.0568 - val_mae: 0.1622 - val_root_mean_squared_error: 0.2383\n",
      "Epoch 37/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0757 - mae: 0.1818 - root_mean_squared_error: 0.2750 - val_loss: 0.0543 - val_mae: 0.1569 - val_root_mean_squared_error: 0.2330\n",
      "Epoch 38/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0797 - mae: 0.1821 - root_mean_squared_error: 0.2819 - val_loss: 0.0557 - val_mae: 0.1684 - val_root_mean_squared_error: 0.2361\n",
      "Epoch 39/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0772 - mae: 0.1797 - root_mean_squared_error: 0.2777 - val_loss: 0.0562 - val_mae: 0.1679 - val_root_mean_squared_error: 0.2372\n",
      "Epoch 40/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0822 - mae: 0.1834 - root_mean_squared_error: 0.2864 - val_loss: 0.0634 - val_mae: 0.1709 - val_root_mean_squared_error: 0.2518\n",
      "Epoch 41/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0778 - mae: 0.1802 - root_mean_squared_error: 0.2787 - val_loss: 0.0670 - val_mae: 0.1838 - val_root_mean_squared_error: 0.2589\n",
      "Epoch 42/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0747 - mae: 0.1774 - root_mean_squared_error: 0.2733 - val_loss: 0.0537 - val_mae: 0.1598 - val_root_mean_squared_error: 0.2317\n",
      "Epoch 43/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0758 - mae: 0.1765 - root_mean_squared_error: 0.2749 - val_loss: 0.0494 - val_mae: 0.1509 - val_root_mean_squared_error: 0.2222\n",
      "Epoch 44/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0664 - mae: 0.1730 - root_mean_squared_error: 0.2573 - val_loss: 0.0615 - val_mae: 0.1705 - val_root_mean_squared_error: 0.2481\n",
      "Epoch 45/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0805 - mae: 0.1822 - root_mean_squared_error: 0.2836 - val_loss: 0.0692 - val_mae: 0.1816 - val_root_mean_squared_error: 0.2631\n",
      "Epoch 46/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0820 - mae: 0.1882 - root_mean_squared_error: 0.2861 - val_loss: 0.0545 - val_mae: 0.1610 - val_root_mean_squared_error: 0.2334\n",
      "Epoch 47/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0745 - mae: 0.1721 - root_mean_squared_error: 0.2722 - val_loss: 0.0595 - val_mae: 0.1691 - val_root_mean_squared_error: 0.2438\n",
      "Epoch 48/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0781 - mae: 0.1780 - root_mean_squared_error: 0.2793 - val_loss: 0.0676 - val_mae: 0.1762 - val_root_mean_squared_error: 0.2600\n",
      "Epoch 49/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0822 - mae: 0.1821 - root_mean_squared_error: 0.2866 - val_loss: 0.0664 - val_mae: 0.1786 - val_root_mean_squared_error: 0.2578\n",
      "Epoch 50/50\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0758 - mae: 0.1782 - root_mean_squared_error: 0.2752 - val_loss: 0.0577 - val_mae: 0.1607 - val_root_mean_squared_error: 0.2402\n",
      "DNN model training complete.\n",
      "\n",
      "Evaluating DNN model on test set...\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "Test Set Evaluation for Deep Neural Network: RMSE = 0.24, R2 = 0.7661, MAE = 0.16\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    history = dnn_model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_test, y_test),\n",
    "        verbose=1, # Show training progress\n",
    "        #callbacks = [early_stopping]\n",
    "    )\n",
    "    print(\"DNN model training complete.\")\n",
    "\n",
    "    print(\"\\nEvaluating DNN model on test set...\")\n",
    "    dnn_eval_results = dnn_model.evaluate(X_test, y_test, verbose=0)\n",
    "    dnn_loss = dnn_eval_results[0]\n",
    "    dnn_rmse = dnn_eval_results[1]\n",
    "    dnn_mae = dnn_eval_results[2]\n",
    "\n",
    "    y_pred_dnn = dnn_model.predict(X_test).flatten()\n",
    "    r2_dnn = r2_score(y_test, y_pred_dnn)\n",
    "\n",
    "    best_models[\"Deep Neural Network\"] = dnn_model\n",
    "    evaluation_results[\"Deep Neural Network\"] = {'RMSE': dnn_rmse, 'R2': r2_dnn, 'MAE': dnn_mae}\n",
    "    print(f\"Test Set Evaluation for Deep Neural Network: RMSE = {dnn_rmse:.2f}, R2 = {r2_dnn:.4f}, MAE = {dnn_mae:.2f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error training DNN: {e}\")\n",
    "    print(\"Skipping DNN model.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b48f0635-f20f-4ae9-b279-45d9d7d4c611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 8. Summary of All Best Models and Test Set Performance ---\n",
      "Model: Stacking Regressor\n",
      "  RMSE: 0.08\n",
      "  R2: 0.9760\n",
      "---\n",
      "Model: XGBoost\n",
      "  RMSE: 0.08\n",
      "  R2: 0.9722\n",
      "---\n",
      "Model: Random Forest\n",
      "  RMSE: 0.08\n",
      "  R2: 0.9720\n",
      "---\n",
      "Model: Decision Tree\n",
      "  RMSE: 0.11\n",
      "  R2: 0.9547\n",
      "---\n",
      "Model: Linear Regression\n",
      "  RMSE: 0.17\n",
      "  R2: 0.8786\n",
      "---\n",
      "Model: Deep Neural Network\n",
      "  RMSE: 0.24\n",
      "  R2: 0.7661\n",
      "  MAE: 0.16\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- 8. Summary of All Best Models and Test Set Performance ---\")\n",
    "sorted_results = sorted(evaluation_results.items(), key=lambda item: item[1]['R2'], reverse=True)\n",
    "\n",
    "for model_name, metrics in sorted_results:\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"  RMSE: {metrics['RMSE']:.2f}\")\n",
    "    print(f\"  R2: {metrics['R2']:.4f}\")\n",
    "    if 'MAE' in metrics:\n",
    "        print(f\"  MAE: {metrics['MAE']:.2f}\")\n",
    "    print(\"---\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "43dd3fa8-b7c1-46cd-8051-93502f170232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 9. Saving Only the XGBoost Model ---\n",
      "Successfully saved XGBoost model to: XGBoost_model.joblib\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- 9. Saving Only the XGBoost Model ---\")\n",
    "\n",
    "model_name_to_save = \"XGBoost\"\n",
    "\n",
    "if model_name_to_save in best_models:\n",
    "    xgboost_model = best_models[model_name_to_save]\n",
    "    safe_model_name = model_name_to_save.replace(\" \", \"_\").replace(\".\", \"\")\n",
    "    model_filename = f\"{safe_model_name}_model.joblib\"\n",
    "\n",
    "    try:\n",
    "        joblib.dump(xgboost_model, model_filename)\n",
    "        print(f\"Successfully saved {model_name_to_save} model to: {model_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving {model_name_to_save} model: {e}\")\n",
    "else:\n",
    "    print(f\"Error: {model_name_to_save} model not found in best_models. Make sure it was trained successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6754542-d655-4b49-9aad-55366371b159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scikit-learn version: 1.6.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\2XIN\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.19.0\n",
      "NumPy version: 2.2.5\n",
      "Pandas version: 2.2.3\n",
      "XGBoost version: 3.0.2\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(\"Scikit-learn version:\", sklearn.__version__)\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "import numpy\n",
    "print(\"NumPy version:\", numpy.__version__)\n",
    "\n",
    "import pandas\n",
    "print(\"Pandas version:\", pandas.__version__)\n",
    "\n",
    "import xgboost\n",
    "print(\"XGBoost version:\", xgboost.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6c5e9df-c5c1-4a9b-a05e-6023f9405cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 9. Saving Only the stack Model ---\n",
      "Successfully saved Stacking Regressor model to: Stacking_Regressor_model.joblib\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- 9. Saving Only the stack Model ---\")\n",
    "\n",
    "model_name_to_save = \"Stacking Regressor\"\n",
    "\n",
    "if model_name_to_save in best_models:\n",
    "    xgboost_model = best_models[model_name_to_save]\n",
    "    safe_model_name = model_name_to_save.replace(\" \", \"_\").replace(\".\", \"\")\n",
    "    model_filename = f\"{safe_model_name}_model.joblib\"\n",
    "\n",
    "    try:\n",
    "        joblib.dump(xgboost_model, model_filename)\n",
    "        print(f\"Successfully saved {model_name_to_save} model to: {model_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving {model_name_to_save} model: {e}\")\n",
    "else:\n",
    "    print(f\"Error: {model_name_to_save} model not found in best_models. Make sure it was trained successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720df8f5-311b-41b4-a334-09d770643ba7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_ENV",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
